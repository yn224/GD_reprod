{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "5f5f9285",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.io\n",
    "import time\n",
    "from scipy.special import expit\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import load_digits\n",
    "from algorithms import *\n",
    "plt.rcParams['figure.figsize'] = [8, 6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "780eefc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'YearPredictionMSD.txt'\n",
    "data = np.loadtxt(filename, delimiter=',', skiprows=1)\n",
    "y = data[:,0]\n",
    "y = y.reshape((len(y), 1))\n",
    "X = data[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "81222148",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Linear_Predict(x,w):\n",
    "    return x@w\n",
    "\n",
    "def Linear_Regression_SGD(x, y, eta, K, L, q=None):\n",
    "    #Initialize weights and bias\n",
    "    b = 0\n",
    "    w = np.zeros([x.shape[1],1])\n",
    "    \n",
    "    costs = []\n",
    "    norm = []\n",
    "    y = y.reshape((len(y),1))\n",
    "    \n",
    "    #For each iteration\n",
    "    for k in tqdm(range(K), disable=tqdmSwitch):\n",
    "        \n",
    "        #Draw random sample with replacement\n",
    "        idx = np.random.randint(0,len(y))\n",
    "        xx = x[idx]\n",
    "        yy = y[idx]\n",
    "        \n",
    "        #Fixed learning rate\n",
    "        a = eta\n",
    "        #a = eta/np.sqrt(k+1)\n",
    "        \n",
    "        #Make prediction\n",
    "        y_pred = xx@w\n",
    "\n",
    "        #Update weights\n",
    "        grad = (xx*(y_pred-yy)).reshape((x.shape[1],1))\n",
    "        w = w - a*(grad + L*w)\n",
    "\n",
    "        #Compute cost\n",
    "        pred = x@w\n",
    "        #norm += [np.linalg.norm(grad)]\n",
    "        costs += [mean_squared_error(y, pred, squared=False)]\n",
    "        \n",
    "    if q != None:\n",
    "        q.put([w, b, np.array(costs)])\n",
    "        \n",
    "    return w, b, np.array(costs)\n",
    "\n",
    "\n",
    "def Linear_Regression_SAG(x, y, eta, K, L, q=None):\n",
    "    #Initialize weights and bias\n",
    "    b = 0\n",
    "    w = np.zeros([x.shape[1],1])\n",
    "    g = np.zeros((x.shape[0], x.shape[1],1)) #Gradient table\n",
    "    G = np.zeros_like(w) #Gradient table sum\n",
    "    idxs = []\n",
    "    m = 0\n",
    "    \n",
    "    costs = []\n",
    "    norm = []\n",
    "    y = y.reshape((len(y),1))\n",
    "    \n",
    "    #For each iteration\n",
    "    for k in tqdm(range(K), disable=tqdmSwitch):\n",
    "        \n",
    "        #Draw random sample with replacement\n",
    "        idx = np.random.randint(0,len(y))\n",
    "        xx = x[idx]\n",
    "        yy = y[idx]\n",
    "        \n",
    "        #Fixed learning rate\n",
    "        a = eta\n",
    "        \n",
    "        #Make prediction\n",
    "        y_pred = xx@w\n",
    "\n",
    "        #Check if data point has been seen\n",
    "        if idx not in idxs:\n",
    "            idxs += [idx]\n",
    "            m += 1\n",
    "        \n",
    "        #Calculate current gradient\n",
    "        grad = (xx*(y_pred-yy)).reshape((x.shape[1],1))\n",
    "        G = G + (grad) - ((g[idx])) \n",
    "        g[idx] = grad\n",
    "\n",
    "        #Update weights\n",
    "        w = w - a*(G/m + L*w)\n",
    "\n",
    "        #Compute cost\n",
    "        pred = x@w\n",
    "        #norm += [np.linalg.norm(grad)]\n",
    "        costs += [mean_squared_error(y, pred, squared=False)]\n",
    "        \n",
    "    if q != None:\n",
    "        q.put([w, b, np.array(costs)])\n",
    "        \n",
    "    return w, b, np.array(costs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "222105f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=463715, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eefb2988",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5af6540f99d144ca8b7d84e41f39e74f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "w1, _, costs1 = Linear_Regression_SGD(X_train, y_train, 10e-10, 100000, 0.001)\n",
    "w2, _, costs2 = Linear_Regression_SAG(X_train, y_train, 10e-10, 100000, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edee767",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(len(costs1)), costs1, label=\"SGD\")\n",
    "plt.plot(np.arange(len(costs2)), costs2, label=\"SAG\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Squared Loss\")\n",
    "plt.title(\"Iterations vs Loss\")\n",
    "plt.legend()\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37f6cfc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
